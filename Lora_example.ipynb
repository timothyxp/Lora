{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install loralib","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:40:01.693419Z","iopub.execute_input":"2021-11-04T12:40:01.693952Z","iopub.status.idle":"2021-11-04T12:40:12.033774Z","shell.execute_reply.started":"2021-11-04T12:40:01.693863Z","shell.execute_reply":"2021-11-04T12:40:12.032837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nimport loralib as lora","metadata":{"id":"5db35d05","execution":{"iopub.status.busy":"2021-11-04T12:40:12.035852Z","iopub.execute_input":"2021-11-04T12:40:12.036119Z","iopub.status.idle":"2021-11-04T12:40:13.303219Z","shell.execute_reply.started":"2021-11-04T12:40:12.036083Z","shell.execute_reply":"2021-11-04T12:40:13.302426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lora_r = 4\n\ndef make_lora_layer(layer):\n    new_layer = lora.Linear(\n        in_features=layer.in_features,\n        out_features=layer.out_features,\n        bias=layer.bias is None,\n        r=lora_r,\n        merge_weights=False\n    )\n    \n    new_layer.weight = nn.Parameter(layer.weight.detach())\n    \n    if layer.bias is not None:\n        new_layer.bias = nn.Parameter(layer.bias.detach())\n    \n    return new_layer\n\n\ndef make_lora_replace(model, depth=1, path=\"\", verbose=True):\n    if depth > 10:\n        return\n    depth += 1\n        \n    if isinstance(model, nn.Linear) and \"attention\" in path:\n        if verbose:\n            print(f\"Find linear {path}:{key} :\", type(module))\n\n        return make_lora_layer(model)\n    \n    for key in dir(model):\n        module = getattr(model, key)\n        module_type = type(module)\n            \n        if not isinstance(module, nn.Module) or module is model:\n            continue\n\n        if isinstance(module, nn.Linear) and \"attention\" in path:\n            layer = make_lora_layer(module)\n            setattr(model, key, layer)\n            if verbose:\n                print(f\"Find linear {path}:{key} :\", type(module))\n            \n        elif isinstance(module, nn.ModuleList):\n            for i, elem in enumerate(module):\n                layer = make_lora_replace(elem, depth, path+\":\"+key+f\"[{i}]\", verbose=verbose)\n                if layer is not None:\n                    module[i] = layer\n                \n        elif isinstance(module, nn.ModuleDict):\n            for module_key in list(module.keys()):\n                layer = make_lora_replace(item, depth, path+\":\"+key+\":\"+module_key, verbose=verbose)\n                if layer is not None:\n                    module[module_key] = layer\n                \n        else:\n            layer = make_lora_replace(module, depth, path+\":\"+key, verbose=verbose)\n            if layer is not None:\n                setattr(model, key, layer)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:40:15.362401Z","iopub.execute_input":"2021-11-04T12:40:15.362939Z","iopub.status.idle":"2021-11-04T12:40:15.478265Z","shell.execute_reply.started":"2021-11-04T12:40:15.362897Z","shell.execute_reply":"2021-11-04T12:40:15.477525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom datasets import load_dataset\nfrom transformers import BertConfig, EncoderDecoderConfig, DataCollatorWithPadding, EncoderDecoderModel, AutoTokenizer, AutoModelForSequenceClassification, CONFIG_MAPPING, AutoConfig","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:40:17.854114Z","iopub.execute_input":"2021-11-04T12:40:17.85465Z","iopub.status.idle":"2021-11-04T12:40:23.388144Z","shell.execute_reply.started":"2021-11-04T12:40:17.854612Z","shell.execute_reply":"2021-11-04T12:40:23.387308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_id, task, tok_train_fold, sentence1_key, num_labels, vocab_len = 'tweet_eval', 'emotion', 'train', 'text', 4, 30000\n\ndataset = load_dataset(dataset_id, task)\n\nUSE_LORA = True\n#model_name = 'roberta-large'\nmodel_name = 'microsoft/deberta-v2-xlarge'\nbatch_size = 16\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ncollator = DataCollatorWithPadding(tokenizer)\n\nif USE_LORA:\n   # first_output = model(**collator([tokenizer('test')]))\n\n    make_lora_replace(model, verbose=True)\n\n    #final_output = model(**collator([tokenizer('test')]))","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:40:37.054541Z","iopub.execute_input":"2021-11-04T12:40:37.055Z","iopub.status.idle":"2021-11-04T12:43:53.000198Z","shell.execute_reply.started":"2021-11-04T12:40:37.054964Z","shell.execute_reply":"2021-11-04T12:43:52.999415Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if USE_LORA:\n    lora.mark_only_lora_as_trainable(model)\n    \n    for name, param in model.named_parameters():\n        if \"deberta\" not in name:\n            print(name)\n            param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:43:53.002165Z","iopub.execute_input":"2021-11-04T12:43:53.002628Z","iopub.status.idle":"2021-11-04T12:43:53.017208Z","shell.execute_reply.started":"2021-11-04T12:43:53.002586Z","shell.execute_reply":"2021-11-04T12:43:53.014096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\n\nwandb.init(project='Lora')","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:43:53.01912Z","iopub.execute_input":"2021-11-04T12:43:53.019316Z","iopub.status.idle":"2021-11-04T12:44:07.950995Z","shell.execute_reply.started":"2021-11-04T12:43:53.019291Z","shell.execute_reply":"2021-11-04T12:44:07.950153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[sentence1_key], truncation=True, padding=True)\n\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n\nencoded_dataset = encoded_dataset.remove_columns(['text'])\n\ndataloaders = {\n    key: DataLoader(ds, shuffle=True, collate_fn=collator, num_workers=2, batch_size=batch_size) for key, ds in encoded_dataset.items()\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:44:17.455038Z","iopub.execute_input":"2021-11-04T12:44:17.455312Z","iopub.status.idle":"2021-11-04T12:44:24.053958Z","shell.execute_reply.started":"2021-11-04T12:44:17.455282Z","shell.execute_reply":"2021-11-04T12:44:24.053305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_loader(dataloader):\n    model.eval()\n    res = []\n    labels = []\n    \n    for batch in tqdm(dataloader):\n        batch = batch_device(batch)\n        output = model(**batch).logits.argmax(dim=-1).detach().cpu().numpy()\n        res.extend(output)\n        labels.extend(batch.labels.detach().cpu().numpy())\n            \n    return res, labels\n\n\ndef batch_device(batch):\n    for key in list(batch.keys()):\n        batch[key] = batch[key].to(device)\n        \n    return batch\n\ndevice = 'cuda'\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:44:26.953132Z","iopub.execute_input":"2021-11-04T12:44:26.953386Z","iopub.status.idle":"2021-11-04T12:44:32.363815Z","shell.execute_reply.started":"2021-11-04T12:44:26.953356Z","shell.execute_reply":"2021-11-04T12:44:32.363027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.watch(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:44:32.367307Z","iopub.execute_input":"2021-11-04T12:44:32.36786Z","iopub.status.idle":"2021-11-04T12:44:32.37917Z","shell.execute_reply.started":"2021-11-04T12:44:32.367828Z","shell.execute_reply":"2021-11-04T12:44:32.378319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom tqdm.autonotebook import tqdm\nfrom sklearn.metrics import f1_score\n\nlr = 2e-5\nif USE_LORA:\n    lr = 2e-4\n\noptimizer = Adam([param for param in model.parameters() if param.requires_grad], lr=lr)\nsteps = len(dataloaders['train'])\nepochs = 15\nscheduler = get_cosine_schedule_with_warmup(optimizer, steps * 1, steps * epochs)\nbest_f1 = 0\n\n\nfor i in range(epochs):\n    model.train()\n    losses = []\n    for batch in tqdm(dataloaders['train']):\n        batch = batch_device(batch)\n        \n        optimizer.zero_grad()\n        output = model(**batch)\n\n        output.loss.backward()\n        \n        wandb.log({\n            \"train/loss\": output.loss.detach().cpu().numpy()\n        })\n        \n        optimizer.step()\n        scheduler.step()\n    \n    res, labels = predict_loader(dataloaders['validation'])\n    res = np.array(res) \n    labels = np.array(labels)\n    f1 = f1_score(labels, res, average='micro')\n    wandb.log({\n        \"eval/f1\": f1\n    })\n    print(f1)\n    \n    if f1 > best_f1:\n        best_f1 = f1\n        checkpoint_path = \"best_lora_checkpoint.pth\"\n        \n        if USE_LORA:\n            torch.save(lora.lora_state_dict(model), checkpoint_path)\n        else:\n            torch.save(model.state_dict(), checkpoint_path)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-04T12:44:48.262191Z","iopub.execute_input":"2021-11-04T12:44:48.262475Z","iopub.status.idle":"2021-11-04T13:30:49.231075Z","shell.execute_reply.started":"2021-11-04T12:44:48.26243Z","shell.execute_reply":"2021-11-04T13:30:49.230263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}